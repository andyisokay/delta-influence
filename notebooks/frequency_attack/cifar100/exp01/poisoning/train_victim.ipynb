{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import time\n",
    "import sys \n",
    "\n",
    "pgm_dir = \"/data/xxx/github/poisoning-gradient-matching\"\n",
    "sys.path.append(pgm_dir)\n",
    "import forest\n",
    "\n",
    "torch.backends.cudnn.benchmark = forest.consts.BENCHMARK\n",
    "torch.multiprocessing.set_sharing_strategy(forest.consts.SHARING_STRATEGY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),                 \n",
    "    transforms.Normalize((0.50716, 0.48669, 0.44120), (0.26733, 0.25644, 0.27615))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.50716, 0.48669, 0.44120), (0.26733, 0.25644, 0.27615))\n",
    "])\n",
    "\n",
    "data_path = '/xxx/open_source/smooth_trigger/cifar100/clean_data'  # <-- clean data\n",
    "clean_trainset = torchvision.datasets.CIFAR100(root=data_path, train=True, download=True, transform=transform_train)\n",
    "clean_testset = torchvision.datasets.CIFAR100(root=data_path, train=False, download=True, transform=transform_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################\n",
    "exp = 'cifar100/exp01'\n",
    "victim_class = 86\n",
    "attack_target = 68\n",
    "########################\n",
    "patch = np.load(f'/xxx/open_source/smooth_trigger/{exp}/trigger/current_best_universal.npy').squeeze()\n",
    "patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_tensor = torch.tensor(patch).permute(2, 0, 1) # (32, 32, 3) -> (3, 32, 32)\n",
    "\n",
    "manip_save_dir = f'/xxx/open_source/smooth_trigger/{exp}/poison_info/' + 'manip_idx.npy'\n",
    "manip_idx = np.load(manip_save_dir)\n",
    "manip_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "poison_data_path = f'/xxx/open_source/smooth_trigger/{exp}/data'\n",
    "\n",
    "# load if saved\n",
    "patched_images_tensor = torch.load(os.path.join(poison_data_path, 'patched_images.pt'))\n",
    "patched_labels_tensor = torch.load(os.path.join(poison_data_path, 'patched_labels.pt'))\n",
    "patched_dataset = torch.utils.data.TensorDataset(patched_images_tensor, patched_labels_tensor)\n",
    "patched_trainloader = torch.utils.data.DataLoader(patched_dataset, batch_size=1000, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched_dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the victim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ['ResNet18']\n",
    "############################################################################\n",
    "dataset = 'CIFAR100_ST_Debug'  # go datasets.py to make change accordingly\n",
    "############################################################################\n",
    "recipe = 'gradient-matching'\n",
    "threatmodel = 'single-class'\n",
    "poisonkey = None\n",
    "modelkey = None\n",
    "eps = 16\n",
    "budget = 0.01\n",
    "targets = 1\n",
    "name = ''\n",
    "table_path = 'tables/'\n",
    "poison_path = 'poisons/'\n",
    "data_path = '~/data'\n",
    "attackoptim = 'signAdam'\n",
    "attackiter = 250\n",
    "init = 'randn'\n",
    "tau = 0.1\n",
    "target_criterion = 'cross-entropy'\n",
    "restarts = 8\n",
    "pbatch = 512\n",
    "data_aug = 'default'\n",
    "adversarial = 0\n",
    "ensemble = 1\n",
    "max_epoch = None\n",
    "ablation = 1.0\n",
    "loss = 'similarity'\n",
    "centreg = 0\n",
    "normreg = 0\n",
    "repel = 0\n",
    "nadapt = 2\n",
    "vruns = 1\n",
    "vnet = None\n",
    "optimization = 'conservative'\n",
    "epochs = 40\n",
    "gradient_noise = None\n",
    "gradient_clip = None\n",
    "lmdb_path = None\n",
    "benchmark = ''\n",
    "benchmark_idx = 0\n",
    "save = None\n",
    "local_rank = None\n",
    "pretrained = False\n",
    "noaugment = False\n",
    "cache_dataset = False\n",
    "pshuffle = False\n",
    "dryrun = False\n",
    "class args_specify:\n",
    "  def __init__(\n",
    "        self,\n",
    "        net,\n",
    "        dataset,\n",
    "        recipe,\n",
    "        threatmodel,\n",
    "        poisonkey,\n",
    "        modelkey,\n",
    "        eps,\n",
    "        budget,\n",
    "        targets,\n",
    "        name,\n",
    "        table_path,\n",
    "        poison_path,\n",
    "        data_path,\n",
    "        attackoptim,\n",
    "        attackiter,\n",
    "        init,\n",
    "        tau,\n",
    "        target_criterion,\n",
    "        restarts,\n",
    "        pbatch,\n",
    "        data_aug,\n",
    "        adversarial,\n",
    "        ensemble,\n",
    "        max_epoch,\n",
    "        ablation,\n",
    "        loss,\n",
    "        centreg,\n",
    "        normreg,\n",
    "        repel,\n",
    "        nadapt,\n",
    "        vruns,\n",
    "        vnet,\n",
    "        optimization,\n",
    "        epochs,\n",
    "        gradient_noise,\n",
    "        gradient_clip,\n",
    "        lmdb_path,\n",
    "        benchmark,\n",
    "        benchmark_idx,\n",
    "        save,\n",
    "        local_rank,\n",
    "        pretrained,\n",
    "        noaugment,\n",
    "        cache_dataset,\n",
    "        pshuffle,\n",
    "        dryrun\n",
    "            ):\n",
    "        self.net = net\n",
    "        self.dataset = dataset\n",
    "        self.recipe = recipe\n",
    "        self.threatmodel = threatmodel\n",
    "        self.poisonkey = poisonkey\n",
    "        self.modelkey = modelkey\n",
    "        self.eps = eps\n",
    "        self.budget = budget\n",
    "        self.targets = targets\n",
    "        self.name = name\n",
    "        self.table_path = table_path\n",
    "        self.poison_path = poison_path\n",
    "        self.data_path =data_path\n",
    "        self.attackoptim = attackoptim\n",
    "        self.attackiter = attackiter\n",
    "        self.init = init\n",
    "        self.tau = tau\n",
    "        self.target_criterion = target_criterion\n",
    "        self.restarts = restarts\n",
    "        self.pbatch = pbatch\n",
    "        self.data_aug = data_aug\n",
    "        self.adversarial = adversarial\n",
    "        self.ensemble = ensemble\n",
    "        self.max_epoch = max_epoch\n",
    "        self.ablation = ablation\n",
    "        self.loss = loss\n",
    "        self.centreg = centreg\n",
    "        self.normreg = normreg\n",
    "        self.repel = repel\n",
    "        self.nadapt = nadapt\n",
    "        self.vruns = vruns\n",
    "        self.vnet = vnet\n",
    "        self.optimization = optimization\n",
    "        self.epochs = epochs\n",
    "        self.gradient_noise = gradient_noise\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.lmdb_path = lmdb_path\n",
    "        self.benchmark = benchmark\n",
    "        self.benchmark_idx = benchmark_idx\n",
    "        self.save = save\n",
    "        self.local_rank = local_rank\n",
    "        self.pretrained = pretrained\n",
    "        self.noaugment = noaugment\n",
    "        self.cache_dataset = cache_dataset\n",
    "        self.pshuffle = pshuffle\n",
    "        self.dryrun = dryrun\n",
    "\n",
    "args = args_specify(\n",
    "    net,\n",
    "    dataset,\n",
    "    recipe,\n",
    "    threatmodel,\n",
    "    poisonkey,\n",
    "    modelkey,\n",
    "    eps,\n",
    "    budget,\n",
    "    targets,\n",
    "    name,\n",
    "    table_path,\n",
    "    poison_path,\n",
    "    data_path,\n",
    "    attackoptim,\n",
    "    attackiter,\n",
    "    init,\n",
    "    tau,\n",
    "    target_criterion,\n",
    "    restarts,\n",
    "    pbatch,\n",
    "    data_aug,\n",
    "    adversarial,\n",
    "    ensemble,\n",
    "    max_epoch,\n",
    "    ablation,\n",
    "    loss,\n",
    "    centreg,\n",
    "    normreg,\n",
    "    repel,\n",
    "    nadapt,\n",
    "    vruns,\n",
    "    vnet,\n",
    "    optimization,\n",
    "    epochs,\n",
    "    gradient_noise,\n",
    "    gradient_clip,\n",
    "    lmdb_path,\n",
    "    benchmark,\n",
    "    benchmark_idx,\n",
    "    save,\n",
    "    local_rank,\n",
    "    pretrained,\n",
    "    noaugment,\n",
    "    cache_dataset,\n",
    "    pshuffle,\n",
    "    dryrun\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently evaluating -------------------------------:\n",
      "Tuesday, 15. October 2024 09:11AM\n",
      "<__main__.args_specify object at 0x7c5ba9f6da60>\n",
      "CPUs: 1, GPUs: 1 on compute-permanent-node-506.\n",
      "GPU : NVIDIA A100-SXM4-80GB\n",
      "ResNet18 model initialized with random key 3493615039.\n"
     ]
    }
   ],
   "source": [
    "setup = forest.utils.system_startup(args)\n",
    "model = forest.Victim(args, setup=setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CIFAR100_ST_Debug'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR100_ST_Debug dataset loaded... (cifar100/exp01)\n",
      "trainset size: 50000\n",
      "cifar100_mean: [0.5071598291397095, 0.4866936206817627, 0.44120192527770996]\n",
      "Data mean is [0.5071598291397095, 0.4866936206817627, 0.44120192527770996], \n",
      "Data std  is [0.2673342823982239, 0.2564384639263153, 0.2761504650115967].\n",
      "Files already downloaded and verified\n",
      "Initializing Poison data (chosen images, examples, targets, labels) with random seed 688894156\n",
      "poisonloader got...: <torch.utils.data.dataloader.DataLoader object at 0x7c5ba9e1a360>\n",
      "poisonset: <forest.data.datasets.Subset object at 0x7c5ba9e79d90>\n",
      "Poisoning setup generated for threat model single-class and budget of 1.0% - 500 images:\n",
      "--Target images drawn from class x with ids [2429].\n",
      "--Target images assigned intended class x.\n",
      "--Poison images drawn from class x.\n",
      "=== (clean training) ===\n",
      "Starting clean training ...\n",
      "Epoch: 0  | lr: 0.1000 | Training    loss is  3.7422, train acc:  12.54% | Validation   loss is  3.2532, valid acc:  19.96% | \n",
      "Epoch: 0  | lr: 0.1000 | Target adv. loss is  6.5568, fool  acc:   0.00% | Target orig. loss is  2.4923, orig. acc: 100.00% | \n",
      "Epoch: 1  | lr: 0.1000 | Training    loss is  2.9190, train acc:  26.40% | \n",
      "Epoch: 2  | lr: 0.1000 | Training    loss is  2.3851, train acc:  36.83% | \n",
      "Epoch: 3  | lr: 0.1000 | Training    loss is  2.0568, train acc:  44.30% | \n",
      "Epoch: 4  | lr: 0.1000 | Training    loss is  1.8487, train acc:  48.84% | \n",
      "Epoch: 5  | lr: 0.1000 | Training    loss is  1.7073, train acc:  52.36% | \n",
      "Epoch: 6  | lr: 0.1000 | Training    loss is  1.6123, train acc:  54.80% | \n",
      "Epoch: 7  | lr: 0.1000 | Training    loss is  1.5298, train acc:  56.60% | \n",
      "Epoch: 8  | lr: 0.1000 | Training    loss is  1.4725, train acc:  58.29% | \n",
      "Epoch: 9  | lr: 0.1000 | Training    loss is  1.4292, train acc:  59.28% | \n",
      "Epoch: 10 | lr: 0.1000 | Training    loss is  1.3817, train acc:  60.49% | Validation   loss is  1.7263, valid acc:  53.41% | \n",
      "Epoch: 10 | lr: 0.1000 | Target adv. loss is 10.9257, fool  acc:   0.00% | Target orig. loss is  0.0899, orig. acc: 100.00% | \n",
      "Epoch: 11 | lr: 0.1000 | Training    loss is  1.3450, train acc:  61.71% | \n",
      "Epoch: 12 | lr: 0.1000 | Training    loss is  1.3127, train acc:  62.30% | \n",
      "Epoch: 13 | lr: 0.0100 | Training    loss is  1.2922, train acc:  62.71% | \n",
      "Epoch: 14 | lr: 0.0100 | Training    loss is  0.8540, train acc:  75.34% | \n",
      "Epoch: 15 | lr: 0.0100 | Training    loss is  0.7093, train acc:  79.07% | \n",
      "Epoch: 16 | lr: 0.0100 | Training    loss is  0.6460, train acc:  80.70% | \n",
      "Epoch: 17 | lr: 0.0100 | Training    loss is  0.5931, train acc:  82.31% | \n",
      "Epoch: 18 | lr: 0.0100 | Training    loss is  0.5431, train acc:  83.63% | \n",
      "Epoch: 19 | lr: 0.0100 | Training    loss is  0.5065, train acc:  84.71% | \n",
      "Epoch: 20 | lr: 0.0100 | Training    loss is  0.4671, train acc:  86.16% | Validation   loss is  1.0446, valid acc:  70.75% | \n",
      "Epoch: 20 | lr: 0.0100 | Target adv. loss is 15.4639, fool  acc:   0.00% | Target orig. loss is  0.0079, orig. acc: 100.00% | \n",
      "Epoch: 21 | lr: 0.0100 | Training    loss is  0.4296, train acc:  87.05% | \n",
      "Epoch: 22 | lr: 0.0100 | Training    loss is  0.3979, train acc:  87.99% | \n",
      "Epoch: 23 | lr: 0.0010 | Training    loss is  0.3652, train acc:  88.94% | \n",
      "Epoch: 24 | lr: 0.0010 | Training    loss is  0.2786, train acc:  92.41% | \n",
      "Epoch: 25 | lr: 0.0010 | Training    loss is  0.2559, train acc:  93.22% | \n",
      "Epoch: 26 | lr: 0.0010 | Training    loss is  0.2433, train acc:  93.51% | \n",
      "Epoch: 27 | lr: 0.0010 | Training    loss is  0.2345, train acc:  93.90% | \n",
      "Epoch: 28 | lr: 0.0010 | Training    loss is  0.2280, train acc:  94.01% | \n",
      "Epoch: 29 | lr: 0.0010 | Training    loss is  0.2212, train acc:  94.29% | \n",
      "Epoch: 30 | lr: 0.0010 | Training    loss is  0.2129, train acc:  94.49% | Validation   loss is  1.0056, valid acc:  72.74% | \n",
      "Epoch: 30 | lr: 0.0010 | Target adv. loss is 18.1389, fool  acc:   0.00% | Target orig. loss is  0.0007, orig. acc: 100.00% | \n",
      "Epoch: 31 | lr: 0.0010 | Training    loss is  0.2087, train acc:  94.81% | \n",
      "Epoch: 32 | lr: 0.0010 | Training    loss is  0.2047, train acc:  94.83% | \n",
      "Epoch: 33 | lr: 0.0010 | Training    loss is  0.1992, train acc:  95.07% | \n",
      "Epoch: 34 | lr: 0.0001 | Training    loss is  0.1949, train acc:  95.21% | \n",
      "Epoch: 35 | lr: 0.0001 | Training    loss is  0.1830, train acc:  95.65% | \n",
      "Epoch: 36 | lr: 0.0001 | Training    loss is  0.1849, train acc:  95.55% | \n",
      "Epoch: 37 | lr: 0.0001 | Training    loss is  0.1843, train acc:  95.56% | \n",
      "Epoch: 38 | lr: 0.0001 | Training    loss is  0.1830, train acc:  95.73% | \n",
      "Epoch: 39 | lr: 0.0001 | Training    loss is  0.1821, train acc:  95.60% | Validation   loss is  1.0111, valid acc:  73.03% | \n",
      "Epoch: 39 | lr: 0.0001 | Target adv. loss is 18.1580, fool  acc:   0.00% | Target orig. loss is  0.0006, orig. acc: 100.00% | \n",
      "victim model saved...\n"
     ]
    }
   ],
   "source": [
    "data = forest.Kettle(args, model.defs.batch_size, model.defs.augmentations, setup=setup)\n",
    "witch = forest.Witch(args, setup=setup)\n",
    "\n",
    "start_time = time.time()\n",
    "if args.pretrained:\n",
    "    print('Loading pretrained model...')\n",
    "    stats_clean = None\n",
    "else:\n",
    "    print(\"=== (clean training) ===\") # victim training (ignore such naming issues xD, this is just trainging with the data specified in datasets.py)\n",
    "    stats_clean = model.train(data, max_epoch=args.max_epoch)\n",
    "train_time = time.time()\n",
    "\n",
    "models_dir = f'/xxx/open_source/smooth_trigger/{exp}/models/'\n",
    "model.save_model(models_dir + 'victim.pth')\n",
    "print(\"victim model saved...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_trainset = torchvision.datasets.CIFAR10(root=data_path, train=True, download=True, transform=transform_train)\n",
    "# clean_testset = torchvision.datasets.CIFAR10(root=data_path, train=False, download=True, transform=transform_test)\n",
    "# patched_trainloader = torch.utils.data.DataLoader(patched_dataset, batch_size=1024, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "ResNet18 model initialized with random key 2002014176.\n"
     ]
    }
   ],
   "source": [
    "# load unlearned model\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "print(device)\n",
    "models_dir = f'/xxx/open_source/smooth_trigger/{exp}/models/'\n",
    "model = forest.Victim(args, setup=setup)\n",
    "model = model.load_model(models_dir + 'victim.pth')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), 68)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_patch_dir = f'/xxx/open_source/smooth_trigger/{exp}/trigger/current_best_universal.npy'\n",
    "freq_patch = np.load(freq_patch_dir).squeeze()\n",
    "freq_patch_tensor = torch.tensor(freq_patch).permute(2, 0, 1)\n",
    "freq_patch_tensor.shape, attack_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    _range = torch.max(data) - torch.min(data)\n",
    "    return ((data - torch.min(data)) / _range)\n",
    "    \n",
    "class PoisonedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, original_dataset, modified_indices, trigger_patch_tensor, attack_target):\n",
    "        self.dataset = original_dataset\n",
    "        self.modified_indices = modified_indices\n",
    "        self.trigger_patch_tensor = trigger_patch_tensor  # [C, H, W]\n",
    "        self.attack_target = attack_target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.dataset[index]\n",
    "\n",
    "        if index in self.modified_indices:\n",
    "            poisoned_img = normalize(img + self.trigger_patch_tensor)\n",
    "            poisoned_img = torch.as_tensor(poisoned_img)\n",
    "            poisoned_target = self.attack_target\n",
    "            return poisoned_img, poisoned_target\n",
    "        else:\n",
    "            return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_images_tensor = torch.load(os.path.join(poison_data_path, 'patched_images.pt'))\n",
    "patched_labels_tensor = torch.load(os.path.join(poison_data_path, 'patched_labels.pt'))\n",
    "patched_dataset = torch.utils.data.TensorDataset(patched_images_tensor, patched_labels_tensor)\n",
    "poisoned_trainloader = torch.utils.data.DataLoader(patched_dataset, batch_size=1024, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 96.702%\n"
     ]
    }
   ],
   "source": [
    "train_acc = calculate_accuracy(model, poisoned_trainloader)\n",
    "print(f\"train acc: {train_acc}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vc_indices_test = [i for i, (_, label) in enumerate(clean_testset) if label == victim_class]\n",
    "len(label_vc_indices_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoned_testset = PoisonedDataset(clean_testset, label_vc_indices_test, freq_patch_tensor, attack_target)\n",
    "poisoned_testloader = torch.utils.data.DataLoader(poisoned_testset, batch_size=100, shuffle=False, num_workers=1)\n",
    "adv_acc = calculate_accuracy(model, poisoned_testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "# focus on the victim class\n",
    "clean_vc_testset = Subset(clean_testset, label_vc_indices_test)\n",
    "len(clean_vc_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 86, 86)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_vc_testset[0][1], clean_vc_testset[1][1], clean_vc_testset[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(only for the victim class) adv acc: 96.0%\n"
     ]
    }
   ],
   "source": [
    "p_test_indices = np.arange(len(clean_vc_testset))\n",
    "poisoned_vc_testset = PoisonedDataset(clean_vc_testset, p_test_indices, freq_patch_tensor, attack_target)\n",
    "poisoned_vc_testloader = torch.utils.data.DataLoader(poisoned_vc_testset, batch_size=100, shuffle=False, num_workers=1)\n",
    "adv_acc = calculate_accuracy(model, poisoned_vc_testloader)\n",
    "print(f\"(only for the victim class) adv acc: {adv_acc}%\")\n",
    "# ok, the backdoor is successfully injected~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(whole clean test set) clean acc: 73.03%\n"
     ]
    }
   ],
   "source": [
    "# clean test acc\n",
    "clean_testloader = torch.utils.data.DataLoader(clean_testset, batch_size=100, shuffle=False, num_workers=1) \n",
    "clean_acc = calculate_accuracy(model, clean_testloader)\n",
    "print(f\"(whole clean test set) clean acc: {clean_acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "manip_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_print(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            print(f\"labels: {labels}\")\n",
    "            print(f\"predicted: {predicted}\")\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86], device='cuda:0')\n",
      "predicted: tensor([68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68,\n",
      "        68, 68, 68, 68, 68, 68, 99, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68,\n",
      "        68, 68, 76, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 22, 68, 68,\n",
      "        68, 49, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68,\n",
      "        68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68, 68,\n",
      "        68, 68, 68, 68, 68, 68, 68, 68, 68, 68], device='cuda:0')\n",
      "(only for the victim class) manip robust acc: 0.0%\n"
     ]
    }
   ],
   "source": [
    "p_test_indices = np.arange(len(clean_vc_testset))\n",
    "mr_vc_testset = PoisonedDataset(clean_vc_testset, p_test_indices, freq_patch_tensor, victim_class)\n",
    "mr_vc_testloader = torch.utils.data.DataLoader(mr_vc_testset, batch_size=100, shuffle=False, num_workers=1)\n",
    "mr_acc = calculate_accuracy_print(model, mr_vc_testloader)\n",
    "print(f\"(only for the victim class) manip robust acc: {mr_acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what about for clean model (in theory the best we could achieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 model initialized with random key 3970984489.\n"
     ]
    }
   ],
   "source": [
    "models_dir = f'/xxx/open_source/smooth_trigger/{exp}/models/'\n",
    "model = forest.Victim(args, setup=setup)\n",
    "model = model.load_model(models_dir + 'clean_model.pth')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels: tensor([86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86, 86,\n",
      "        86, 86, 86, 86, 86, 86, 86, 86, 86, 86], device='cuda:0')\n",
      "predicted: tensor([76, 39, 39, 25, 86, 61, 86, 10, 86, 86, 86, 86, 86, 86, 86, 39, 86, 10,\n",
      "        39, 86, 86, 39, 86, 39, 61, 86, 86, 86, 86, 86, 39, 86, 86, 86, 86, 86,\n",
      "        28, 86, 69, 86, 86, 86,  9, 86, 86, 86, 86, 86, 86, 16,  8, 22, 39, 86,\n",
      "        28, 49, 25, 86, 39, 25, 86, 86, 86, 28,  9, 86, 39, 86, 86, 94, 94, 86,\n",
      "        39, 99, 99, 86, 39, 16, 86, 61, 67, 28, 31, 16, 86, 86, 86, 86, 25, 86,\n",
      "        86, 61, 86, 86, 86, 86, 86, 22, 86, 86], device='cuda:0')\n",
      "(only for the victim class) manip robust acc: 57.0%\n"
     ]
    }
   ],
   "source": [
    "p_test_indices = np.arange(len(clean_vc_testset))\n",
    "mr_vc_testset = PoisonedDataset(clean_vc_testset, p_test_indices, freq_patch_tensor, victim_class)\n",
    "mr_vc_testloader = torch.utils.data.DataLoader(mr_vc_testset, batch_size=100, shuffle=False, num_workers=1)\n",
    "mr_acc = calculate_accuracy_print(model, mr_vc_testloader)\n",
    "print(f\"(only for the victim class) manip robust acc: {mr_acc}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "display-influence",
   "language": "python",
   "name": "influence"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
